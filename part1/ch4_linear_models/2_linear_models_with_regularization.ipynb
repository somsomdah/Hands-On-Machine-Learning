{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=100 # m개의 샘플\n",
    "X=6*np.random.rand(m,1)-3\n",
    "y=0.5*X**2+X+2+np.random.randn(m,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.5규제가 있는 선형 모델\n",
    "- Overfitting을 방지하기 위해 모델을 규제하는 것\n",
    "- 선형 회귀 모델에서는 가중체를 제한함으로써 규제를 가한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.1 릿지 회귀 Ridge regression\n",
    "- L2 regression\n",
    "- 비용함수에 규제항이 추가된 선형 회귀 모델\n",
    "$$J(\\theta)=MSE(\\theta)+\\alpha{1\\over 2} \\sum_{i=1}^n{\\theta_i^2}$$\n",
    "- 규제는 비용함수에 추가되기 때문에 테스트세트에서 성능을 평가하거나 실제 샘플을 예측할 때 포함되지 않는다\n",
    "- 릿지 회귀는 입력 특성의 스케일에 민감하기 때문에 데이터의 스케일을 맞춰주는 것이 중요\n",
    "- $\\alpha$가 증가할수록 모델의 분산은 줄지만 편향은 커진다 (규제가 커진다)\n",
    "- 릿지회귀를 계산하기 위해 정규방정식을 사용할 수 있다. A는 단위행렬에서 첫번째 원소만 1로 바꾼 것\n",
    "$$ \\hat{\\theta}=(X^T \\cdot X+\\alpha A)^{-1} \\cdot X^T \\cdot y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge_reg=Ridge(alpha=1,solver=\"cholesky\")\n",
    "ridge_reg=fit(X,y)\n",
    "print(ridge_reg.predict([[1.5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frim sklearn.linear_model import SGDRegressor\n",
    "sgd_reg=SGDRegressor(penalty='l2')\n",
    "sgd_reg.fit(X,y)\n",
    "print(sgd_reg.predict([[1.5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.2 라쏘 회귀 Lasso Regression\n",
    "- L1 regression\n",
    "- 비용함수에 규제항이 추가된 선형 회귀 모델\n",
    "$$J(\\theta)=MSE(\\theta)+\\alpha \\sum_{i=1}^n{\\left\\vert \\theta_i \\right\\vert}$$\n",
    "- 덜 중요한 특성의 가중치를 완전히 제거하려고 한다(0으로 만든다)\n",
    "- $\\alpha$가 커질수록 다항회귀 곡선은 차수가 줄어들어 직선에 가까워진다. 죽 고차항의 계수를 삭제한다\n",
    "- 즉, 라쏘 회귀는 자동으로 특성 선택을 하고 희소 모델을 만든다.\n",
    "- 라쏘의 비용함수는 0에서 미분 불가능하지만 subgradient vector를 사용하면 경사하강법을 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lasso_reg = Lasso(alpha=0.1)\n",
    "lasso_reg.fit(X, y)\n",
    "lasso_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_reg=SGDRegressor(penalty='l1')\n",
    "sgd_reg.fit(X,y)\n",
    "print(sgd_reg.predict([[1.5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.3 엘라스틱넷 Elastic net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.4 조기종료 early stopping"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
